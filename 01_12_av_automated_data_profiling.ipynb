{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1. Automated Data Profiling In Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author : Anandakumar Varatharajah**\n",
    "<br>\n",
    "***http://www.analyticsinsights.ninja***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version   : 0.11  \n",
    "Date      : 21 May 2019  \n",
    "License   : MIT License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main objective of this notebook is **only** to understand raw data profile. i.e. data type, min & max values, ranges, unique values, etc.  \n",
    "In consequent notebooks we will explore further on how to make decisions to make the data tidy and perform the data transformations based on the understanding of the data profile.\n",
    "<br>\n",
    "The code is largely kept generic so that it could be used with any shape of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Game Changer - Data Profile Dataframe (DPD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game changer for exploratory data analysis is the final ***Data Profile Dataframe*** that is generated which combines ***all*** the information required to inform data cleaning, tidy data and optimisations (memory and processing) decisions.  \n",
    "Instead of using various Pandas commands at different instances and going back and forth to cross refer information, Data Profile Dataframe brings all information into a single dataframe. This will be very useful when reviewing the data profile with the business subject matter or other team members as all information related to data profile is in a single easy to understand format.\n",
    "\n",
    "![image.png](DPD_image_small.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the data is **the critical step** in preparing the data to be used for analytics. As many experts will point out the data preparation and transforming the data into a tidy format takes about 80% of the effort in any data analytics or data analysis project.<br>\n",
    "***Understanding the data requires good understanding of the domain and/or access to a subject matter expert (SME) to help make decisions about data quality and data usage:***\n",
    "* What are the columns and what do they mean?\n",
    "* How to interpret each columns and possible values of a column?\n",
    "* Should the columns be renamed (and cleaned e.g. trim)?\n",
    "* Are there columns that may have similar information that could be dropped in favour of one master column?\n",
    "* Can columns with no values (or all empty) be dropped?\n",
    "* Can columns which have more than certain threshold of blank values be dropped?\n",
    "* How can the missing values be filled and can it be filled meaningfully?\n",
    "* Can rows that have missing values for certain columns or combination of columns be dropped? i.e. the row is meaningless wihtout those values.\n",
    "* Can the numeric data type columns be converted / down casted to optimise memory usage based on the data values?\n",
    "    - or will there be outliers possibly in future data sets that we cannot do this?\n",
    "    - can the min and max values be used to determine the lowest possible data type?\n",
    "* Can some string/object columns be converted to Category types?\n",
    "    - based on count of unique values\n",
    "* Can any columns be discarded that may not be required for analytics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is recommended best practice to document the execution environment.  \n",
    "e.g. When the initial version of this notebook was developed in Azure Notebooks (Jupyter) the environment was documented in the code. When the notebook was exported to local PC JupyterLab and then imported back into Azure Notebook, the Kernal changed to an older version and some code did not work. Having the initital versions documented in comments saved a lot of effort in trying to understand what went wrong.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the date of execution\n",
    "import datetime\n",
    "date_generated = datetime.datetime.now()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version \n",
    "# use python_version() to get the version. This is used in the final DPD HTML\n",
    "# 3.6.6 in Azure Notebooks in April 2019\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# use pd.__version__ to get the pandas version. This is used in the final DPD HTML\n",
    "# Pandas version   0.22.0 in Azure Notebooks in April 2019\n",
    "\n",
    "# set maximum number of columns to display in notebook\n",
    "pd.set_option('display.max_columns', 250)\n",
    "\n",
    "# To check whether a column is numeric type\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "# To check whether a column is object/string type\n",
    "from pandas.api.types import is_string_dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import HTML to display the final DPD HTML table or any HTML code\n",
    "from IPython.display import HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the graph packages\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "%matplotlib inline\n",
    "\n",
    "# Seabotn version   0.9.0 in Azure Notebooks in April 2019\n",
    "# use sns.__version__ to get the pandas version. This is used in the final DPD HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  data_profile_df_HTML  already exists\n"
     ]
    }
   ],
   "source": [
    "# Create the directories for images and the DPD HTML file\n",
    "# if they do not already exists \n",
    "import os\n",
    "\n",
    "# Create directory\n",
    "dirName = 'data_profile_df_HTML'\n",
    "\n",
    "try:\n",
    "    # Create target Directory\n",
    "    os.mkdir(dirName)\n",
    "    print(\"Directory \" , dirName ,  \" Created \") \n",
    "except FileExistsError:\n",
    "    print(\"Directory \" , dirName ,  \" already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw data file exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw data file used in this notebook has been derived from the Sales Products csv file from IBM Analytics Community and has been modified to include untidy data for the purposes of this data exploration work.  \n",
    "The raw data should be in a format that can be laoded into pandas. i.e. if there are any rows need to be skipped,  column headers mapped, etc. should be handle in the pandas.read code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data file to be loaded\n",
    "raw_data_file = \"https://raw.githubusercontent.com/AnalyticsInsightsNinja/Sample_Analytics_Data/master/flightdata.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Pandas to load the data file into a dataframe\n",
    "try:\n",
    "    df = pd.read_csv(raw_data_file, thousands=',', float_precision=2)\n",
    "except:\n",
    "    print(\"Error: Data file not found!\")\n",
    "#     import sys\n",
    "#     sys.exit(\"ERROR: File not found!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If the raw data is a big data file of several GB's in size it may not be possible to load the the whole file into memory. One possibility is using 'pandas pyspark'.<br>\n",
    "Other options to load data incrementally and optimise the data by converting data types will be demonstrated in a seperate notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>QUARTER</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY_OF_MONTH</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>UNIQUE_CARRIER</th>\n",
       "      <th>TAIL_NUM</th>\n",
       "      <th>FL_NUM</th>\n",
       "      <th>ORIGIN_AIRPORT_ID</th>\n",
       "      <th>ORIGIN</th>\n",
       "      <th>DEST_AIRPORT_ID</th>\n",
       "      <th>DEST</th>\n",
       "      <th>CRS_DEP_TIME</th>\n",
       "      <th>DEP_TIME</th>\n",
       "      <th>DEP_DELAY</th>\n",
       "      <th>DEP_DEL15</th>\n",
       "      <th>CRS_ARR_TIME</th>\n",
       "      <th>ARR_TIME</th>\n",
       "      <th>ARR_DELAY</th>\n",
       "      <th>ARR_DEL15</th>\n",
       "      <th>CANCELLED</th>\n",
       "      <th>DIVERTED</th>\n",
       "      <th>CRS_ELAPSED_TIME</th>\n",
       "      <th>ACTUAL_ELAPSED_TIME</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>Unnamed: 25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1765</th>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>DL</td>\n",
       "      <td>N681DA</td>\n",
       "      <td>1700</td>\n",
       "      <td>14747</td>\n",
       "      <td>SEA</td>\n",
       "      <td>13487</td>\n",
       "      <td>MSP</td>\n",
       "      <td>35</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>551</td>\n",
       "      <td>542.0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>1399.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10174</th>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>DL</td>\n",
       "      <td>N3750D</td>\n",
       "      <td>2193</td>\n",
       "      <td>13487</td>\n",
       "      <td>MSP</td>\n",
       "      <td>14747</td>\n",
       "      <td>SEA</td>\n",
       "      <td>701</td>\n",
       "      <td>700.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>859</td>\n",
       "      <td>826.0</td>\n",
       "      <td>-33.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>1399.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11115</th>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>DL</td>\n",
       "      <td>N927AT</td>\n",
       "      <td>421</td>\n",
       "      <td>11433</td>\n",
       "      <td>DTW</td>\n",
       "      <td>12478</td>\n",
       "      <td>JFK</td>\n",
       "      <td>1955</td>\n",
       "      <td>1954.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2145</td>\n",
       "      <td>2153.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>509.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2575</th>\n",
       "      <td>2016</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>DL</td>\n",
       "      <td>N703TW</td>\n",
       "      <td>433</td>\n",
       "      <td>12478</td>\n",
       "      <td>JFK</td>\n",
       "      <td>10397</td>\n",
       "      <td>ATL</td>\n",
       "      <td>610</td>\n",
       "      <td>609.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>844</td>\n",
       "      <td>836.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>760.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1084</th>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>DL</td>\n",
       "      <td>N934DL</td>\n",
       "      <td>996</td>\n",
       "      <td>10397</td>\n",
       "      <td>ATL</td>\n",
       "      <td>12478</td>\n",
       "      <td>JFK</td>\n",
       "      <td>1000</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1211</td>\n",
       "      <td>1202.0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>760.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       YEAR  QUARTER  MONTH  DAY_OF_MONTH  DAY_OF_WEEK UNIQUE_CARRIER  \\\n",
       "1765   2016        1      3             2            3             DL   \n",
       "10174  2016        4     11            30            3             DL   \n",
       "11115  2016        4     12            29            4             DL   \n",
       "2575   2016        2      4             6            3             DL   \n",
       "1084   2016        1      2            12            5             DL   \n",
       "\n",
       "      TAIL_NUM  FL_NUM  ORIGIN_AIRPORT_ID ORIGIN  DEST_AIRPORT_ID DEST  \\\n",
       "1765    N681DA    1700              14747    SEA            13487  MSP   \n",
       "10174   N3750D    2193              13487    MSP            14747  SEA   \n",
       "11115   N927AT     421              11433    DTW            12478  JFK   \n",
       "2575    N703TW     433              12478    JFK            10397  ATL   \n",
       "1084    N934DL     996              10397    ATL            12478  JFK   \n",
       "\n",
       "       CRS_DEP_TIME  DEP_TIME  DEP_DELAY  DEP_DEL15  CRS_ARR_TIME  ARR_TIME  \\\n",
       "1765             35      35.0        0.0        0.0           551     542.0   \n",
       "10174           701     700.0       -1.0        0.0           859     826.0   \n",
       "11115          1955    1954.0       -1.0        0.0          2145    2153.0   \n",
       "2575            610     609.0       -1.0        0.0           844     836.0   \n",
       "1084           1000    1007.0        7.0        0.0          1211    1202.0   \n",
       "\n",
       "       ARR_DELAY  ARR_DEL15  CANCELLED  DIVERTED  CRS_ELAPSED_TIME  \\\n",
       "1765        -9.0        0.0        0.0       0.0             196.0   \n",
       "10174      -33.0        0.0        0.0       0.0             238.0   \n",
       "11115        8.0        0.0        0.0       0.0             110.0   \n",
       "2575        -8.0        0.0        0.0       0.0             154.0   \n",
       "1084        -9.0        0.0        0.0       0.0             131.0   \n",
       "\n",
       "       ACTUAL_ELAPSED_TIME  DISTANCE  Unnamed: 25  \n",
       "1765                 187.0    1399.0          NaN  \n",
       "10174                206.0    1399.0          NaN  \n",
       "11115                119.0     509.0          NaN  \n",
       "2575                 147.0     760.0          NaN  \n",
       "1084                 115.0     760.0          NaN  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample raw data rows from dataset\n",
    "df.sample(5).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Usage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the file is obatined from url.\n",
    "# If from url, then skip file size in disk check\n",
    "\n",
    "if \"http\" in raw_data_file:\n",
    "    file_size = float('nan')\n",
    "else:\n",
    "    # Calculating file size (in MB) on disk\n",
    "    import os\n",
    "\n",
    "    file_size = (os.stat(raw_data_file).st_size / 1024 **2)\n",
    "    #This is used in the DPD HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate dataset size in memory (MB)\n",
    "df_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "#This is used in the DPD HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calclulate dataset size increase in memory (MB)\n",
    "sz_increase = ((df_mem - file_size) / file_size)\n",
    "#This is used in the DPD HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the memory usage \n",
    "# Create a dictionary from the variables and convert to Pandas DataFrame\n",
    "# Use DataFrame's ploting capabilities\n",
    "raw_data_dict = {\"File on disk\":file_size, \"Dataset in memroy\": df_mem}\n",
    "raw_data_plot = pd.DataFrame.from_dict(raw_data_dict, orient='index').reset_index()\n",
    "\n",
    "# Pandas DataFrame plot\n",
    "raw_data_plot.plot(kind='bar',\\\n",
    "                   x=\"index\" ,\\\n",
    "                   y=0, \\\n",
    "                   legend=False, \\\n",
    "                   title='Data size increase from disk to memory')\n",
    "# plt.subplots_adjust(wspace=0.4, hspace=0.35)\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('data_profile_df_HTML/fig_df_tot_memory.png', dpi=50)\n",
    "plt.close('all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get memory used by each column in the raw data dataset in MB\n",
    "# This will be later merged with the DPD\n",
    "mem_used_dtypes = pd.DataFrame(df.memory_usage(deep=True) / 1024**2)\n",
    "\n",
    "# Rename column\n",
    "mem_used_dtypes.rename(columns={ 0:'memory'}, inplace=True)\n",
    "\n",
    "# Drop index memory usage since this is not required when merging with Data Quality Dataframe\n",
    "mem_used_dtypes.drop('Index', axis=0, inplace=True) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing The Data Profile Dataframe (DPD) - The Game Changer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of rows of the DPD will be the count of columns in the raw date dataframe\n",
    "# Since it there will be one row for each column\n",
    "no_of_rows = len(df.columns)\n",
    "\n",
    "\n",
    "# Constructing the data_qlt_df dataframe and pre-assigning and columns\n",
    "# Pre-assigning the number of rows the dataframe would have is memory and processing efficient\n",
    "# This is a better approach than continuous append or concat operation to dataframe\n",
    "\n",
    "data_qlt_df = pd.DataFrame(index=np.arange(0, no_of_rows), \\\n",
    "                            columns=('column_name', 'col_data_type', 'col_memory','non_null_values', \\\n",
    "                                     'unique_values_count', 'column_dtype')\n",
    "                          )\n",
    "\n",
    "\n",
    "# Add rows to the data_qlt_df dataframe\n",
    "for ind, cols in enumerate(df.columns):\n",
    "    # Count of unique values in the column\n",
    "    col_unique_count = df[cols].nunique()\n",
    "    \n",
    "    data_qlt_df.loc[ind] = [cols, \\\n",
    "                            df[cols].dtype, \\\n",
    "                            mem_used_dtypes['memory'][ind], \\\n",
    "                            df[cols].count(), \\\n",
    "                            col_unique_count, \\\n",
    "                            cols + '~'+ str(df[cols].dtype)\n",
    "                            ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use describe() to get column stats of raw dataframe\n",
    "# This will be merged with the DPD\n",
    "raw_num_df = df.describe().T.round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----- Key Step ---------------\n",
    "# Merging the df.describe() output with rest of the info to create a single Data Profile Dataframe\n",
    "data_qlt_df = pd.merge(data_qlt_df, raw_num_df, how='left', left_on='column_name', right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentage of non-null values over total number of values\n",
    "data_qlt_df['%_of_non_nulls'] = (data_qlt_df['non_null_values']/df.shape[0])*100\n",
    "\n",
    "# Calculate null values for the column\n",
    "data_qlt_df['null_values'] = df.shape[0] - data_qlt_df['non_null_values']\n",
    "\n",
    "# Calculate percentage of null values over total number of values\n",
    "data_qlt_df['%_of_nulls'] = 100 - data_qlt_df['%_of_non_nulls']\n",
    "\n",
    "# Calculate percentage of each column memory usage compared to total memory used by raw data datframe\n",
    "data_qlt_df['%_of_total_memory'] = data_qlt_df['col_memory'] / data_qlt_df['col_memory'].sum() * 100\n",
    "\n",
    "# Calculate the total memory used by a given group of data type\n",
    "# See Notes section at the bottom of this notebook for advatages of using 'transform' function with group_by\n",
    "data_qlt_df[\"dtype_total\"] = data_qlt_df.groupby('col_data_type')[\"col_memory\"].transform('sum')\n",
    "\n",
    "# Calculate the percentage memory used by each column data type compared to the total memory used by the group of data type\n",
    "# the above can be merged to one calculation if we do not need the total as separate column\n",
    "#data_qlt_df[\"%_of_dtype_mem2\"] = data_qlt_df[\"Dtype Memory\"] / (data_qlt_df.groupby('Data Type')[\"Dtype Memory\"].transform('sum')) * 100\n",
    "data_qlt_df[\"%_of_dtype_mem\"] = data_qlt_df[\"col_memory\"] / data_qlt_df[\"dtype_total\"] * 100\n",
    "\n",
    "# Calculate the percentage memory used by each group of data type of the total memory used by dataset\n",
    "data_qlt_df[\"dtype_%_total_mem\"] = data_qlt_df[\"dtype_total\"] / df_mem * 100\n",
    "\n",
    "# Calculate the count of each data type\n",
    "data_qlt_df[\"dtype_count\"] = data_qlt_df.groupby('col_data_type')[\"col_data_type\"].transform('count')\n",
    "\n",
    "# Calculate the total count of column values\n",
    "data_qlt_df[\"count\"] = data_qlt_df['null_values'] + data_qlt_df['non_null_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder the Data Profile Dataframe columns\n",
    "data_qlt_df = data_qlt_df[\n",
    "                            ['column_name', 'col_data_type', 'col_memory', '%_of_dtype_mem', '%_of_total_memory',\\\n",
    "                             'dtype_count', 'dtype_total', 'dtype_%_total_mem', 'non_null_values', '%_of_non_nulls',\\\n",
    "                             'null_values', '%_of_nulls', 'unique_values_count', 'count', 'mean', 'std', 'min', '25%',\\\n",
    "                             '50%', '75%', 'max']\n",
    "                         ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The above data quality data frame summarises all information required for making data quality decisions.**  \n",
    "Though there are info() and describe() methods to do these, having all the relvant information in one dataframe makes the data quality exploration much easier. This dataframe can be used for summarising information and for plotting to ehnace the ease of Data Understanding effort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Memory Usage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot count of column data types and memory used by each datatype\n",
    "plt_dtype = data_qlt_df.groupby('col_data_type')['dtype_count', 'dtype_total', 'dtype_%_total_mem'].last().sort_values(by='dtype_count')\n",
    "\n",
    "fig1, (ax, ax2) = plt.subplots(ncols=2, figsize=(10,5))\n",
    "plt_dtype.plot(kind='bar', y='dtype_count',  use_index=True, legend=False, ax=ax, title='Count of columns by data type')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt_dtype.plot(kind='bar', y='dtype_total',  use_index=True, legend=False, ax=ax2, title='Memory used by data type')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "fig1.savefig(\"data_profile_df_HTML/fig_cols_memory.png\", dpi=50)\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory used by columns of raw data dataframe\n",
    "fig2, ax = plt.subplots(ncols=1, figsize=(15,5))\n",
    "\n",
    "# Memory used by object data type\n",
    "(data_qlt_df[data_qlt_df['col_data_type'] == 'object']\n",
    " .sort_values(by='col_memory', ascending=False)\n",
    " .plot(kind=\"bar\", \n",
    "       x=\"column_name\", \n",
    "       y=\"col_memory\", \n",
    "       title=\"Memory (MB) usage by columns of object data type\",\n",
    "      legend=False, ax=ax)\n",
    ")\n",
    "\n",
    "fig2.savefig(\"data_profile_df_HTML/fig_object_cols_memory.png\", dpi=50)\n",
    "plt.close('all')\n",
    "\n",
    "# Memory used by non-object data type\n",
    "fig2, ax1 = plt.subplots(ncols=1, figsize=(15,5))\n",
    "(data_qlt_df[data_qlt_df['col_data_type'] != 'object']\n",
    " .sort_values(by='col_memory', ascending=False)\n",
    " .plot(kind=\"bar\", \n",
    "       x=\"column_name\", \n",
    "       y=\"col_memory\", \n",
    "       title=\"Memory (MB) usage by columns of non-object data type\",\n",
    "      legend=False, ax=ax1)\n",
    ")\n",
    "\n",
    "fig2.savefig(\"data_profile_df_HTML/fig_non_object_cols_memory.png\", dpi=50)\n",
    "plt.close('all')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data profile graphs for 'numerical' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  of  21  completed    YEAR\n",
      "2  of  21  completed    QUARTER\n",
      "3  of  21  completed    MONTH\n",
      "4  of  21  completed    DAY_OF_MONTH\n",
      "5  of  21  completed    DAY_OF_WEEK\n",
      "6  of  21  completed    FL_NUM\n",
      "7  of  21  completed    ORIGIN_AIRPORT_ID\n",
      "8  of  21  completed    DEST_AIRPORT_ID\n",
      "9  of  21  completed    CRS_DEP_TIME\n",
      "10  of  21  completed    DEP_TIME\n",
      "11  of  21  completed    DEP_DELAY\n",
      "12  of  21  completed    DEP_DEL15\n",
      "13  of  21  completed    CRS_ARR_TIME\n",
      "14  of  21  completed    ARR_TIME\n",
      "15  of  21  completed    ARR_DELAY\n",
      "16  of  21  completed    ARR_DEL15\n",
      "17  of  21  completed    CANCELLED\n",
      "18  of  21  completed    DIVERTED\n",
      "19  of  21  completed    CRS_ELAPSED_TIME\n",
      "20  of  21  completed    ACTUAL_ELAPSED_TIME\n",
      "21  of  21  completed    DISTANCE\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# Get the list of numeric columns from raw dataframe\n",
    "# need this: from pandas.api.types import is_numeric_dtype\n",
    "# get numeric columns which are not empty\n",
    "num_cols = [cols for cols in df.columns if is_numeric_dtype(df[cols]) and len(df[cols].dropna())>0]\n",
    "\n",
    "iter_len = len(num_cols)\n",
    "\n",
    "# For each numeric column in the list\n",
    "for x, col_name in enumerate(num_cols):\n",
    "    print(x+1, \" of \", iter_len, \" completed   \",  col_name)\n",
    "    \n",
    "    # Create a copy of the column values without nulls or NA\n",
    "    no_null_col = df[col_name].dropna()\n",
    "    \n",
    "    \n",
    "    # Calculate the 95 percentile of the values\n",
    "    q25 = np.percentile(no_null_col, 25)\n",
    "    q75 = np.percentile(no_null_col, 75)    \n",
    "    q95 = np.percentile(no_null_col, 95)\n",
    "    \n",
    "    # Plot the graphs\n",
    "    fig3 = plt.figure(figsize=(20,15))\n",
    "    fig3.suptitle(\"Profile of column  \" + col_name, fontsize=25)  #Title for the whole figure\n",
    "    plt.subplots_adjust(wspace=0.4, hspace=0.35)\n",
    "\n",
    "    ax1 = fig3.add_subplot(2,3,1)\n",
    "    ax1.set_title(\"Box plot for all the values\", fontsize=20)\n",
    "    plt.setp(ax1.get_xticklabels(), ha=\"right\", rotation=35)\n",
    "    plt.setp(ax1.get_yticklabels(), ha=\"right\", fontsize=15)\n",
    "    ax1.boxplot(no_null_col)\n",
    "\n",
    "    ax1 = fig3.add_subplot(2,3,2)\n",
    "    ax1.set_title(\"Distribution of all values\", fontsize=20)\n",
    "    plt.setp(ax1.get_xticklabels(), ha=\"right\", rotation=35, fontsize=15)\n",
    "    plt.setp(ax1.get_yticklabels(), ha=\"right\", fontsize=15)\n",
    "    ax1.hist(no_null_col)\n",
    "\n",
    "    ax1 = fig3.add_subplot(2,3,3)\n",
    "    ax1.set_title(\"Boxplot for quartiles (all values)\", fontsize=20)\n",
    "    if len(no_null_col.value_counts()) >= 4:\n",
    "        df[u'quartiles'] = pd.qcut(\n",
    "                        df[col_name],\n",
    "                        4, duplicates='drop')\n",
    "        df.boxplot(column= col_name, by=u'quartiles', ax = ax1)\n",
    "    plt.setp(ax1.get_xticklabels(), ha=\"right\", rotation=35, fontsize=15)\n",
    "    plt.setp(ax1.get_yticklabels(), ha=\"right\", fontsize=15)\n",
    "\n",
    "    ax1 = fig3.add_subplot(2,3,4)\n",
    "    ax1.set_title(\"Box plot without outliers\", fontsize=20)\n",
    "    plt.setp(ax1.get_xticklabels(), ha=\"right\", rotation=35, fontsize=15)\n",
    "    plt.setp(ax1.get_yticklabels(), ha=\"right\", fontsize=15)\n",
    "    ax1.boxplot(no_null_col, showfliers=False)\n",
    "\n",
    "    ax1 = fig3.add_subplot(2,3,5)\n",
    "    ax1.set_title(\"Violin plot (<95% percentile)\", fontsize=20)\n",
    "    plt.setp(ax1.get_xticklabels(), ha=\"right\", rotation=35, fontsize=15)\n",
    "    plt.setp(ax1.get_yticklabels(), ha=\"right\", fontsize=15)\n",
    "    ax1.violinplot(no_null_col[no_null_col <= q95])\n",
    "\n",
    "    \n",
    "    #Histogram with bin ranges, counts and percentile color\n",
    "    ax1 = fig3.add_subplot(2,3,6)\n",
    "    ax1.set_title(\"Histogram (<95% percentile)\", fontsize=20)\n",
    "    plt.setp(ax1.get_xticklabels(), ha=\"right\", rotation=35, fontsize=15)\n",
    "    plt.setp(ax1.get_yticklabels(), ha=\"right\", fontsize=15)\n",
    "\n",
    "    # Take only the data less than 95 percentile\n",
    "    data = no_null_col[no_null_col <= q95]\n",
    "\n",
    "    # Colours for different percentiles\n",
    "    perc_25_colour = 'gold'\n",
    "    perc_50_colour = 'mediumaquamarine'\n",
    "    perc_75_colour = 'deepskyblue'\n",
    "    perc_95_colour = 'peachpuff'\n",
    "\n",
    "    '''\n",
    "    counts  = numpy.ndarray of count of data ponts for each bin/column in the histogram\n",
    "    bins    = numpy.ndarray of bin edge/range values\n",
    "    patches = a list of Patch objects.\n",
    "            each Patch object contains a Rectnagle object. \n",
    "            e.g. Rectangle(xy=(-2.51953, 0), width=0.501013, height=3, angle=0)\n",
    "    '''\n",
    "    counts, bins, patches = ax1.hist(data, bins=10, facecolor=perc_50_colour, edgecolor='gray')\n",
    "\n",
    "    # Set the ticks to be at the edges of the bins.\n",
    "    ax1.set_xticks(bins.round(2))\n",
    "    plt.xticks(rotation=70, fontsize=15)\n",
    "\n",
    "    # Change the colors of bars at the edges\n",
    "    for patch, leftside, rightside in zip(patches, bins[:-1], bins[1:]):\n",
    "        if rightside < q25:\n",
    "            patch.set_facecolor(perc_25_colour)\n",
    "        elif leftside > q95:\n",
    "            patch.set_facecolor(perc_95_colour)\n",
    "        elif leftside > q75:\n",
    "            patch.set_facecolor(perc_75_colour)\n",
    "\n",
    "    # Calculate bar centre to display the count of data points and %\n",
    "    bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
    "    bin_y_centers = ax1.get_yticks()[1] * 0.25\n",
    "\n",
    "    # Display the the count of data points and % for each bar in histogram\n",
    "    for i in range(len(bins)-1):\n",
    "        bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
    "        plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
    "\n",
    "    #create legend\n",
    "    handles = [Rectangle((0,0),1,1,color=c,ec=\"k\") for c in [perc_25_colour, perc_50_colour, perc_75_colour, perc_95_colour]]\n",
    "    labels= [\"0-25 Percentile\",\"25-50 Percentile\", \"50-75 Percentile\", \">95 Percentile\"]\n",
    "    plt.legend(handles, labels, bbox_to_anchor=(0.5, 0., 0.85, 0.99))\n",
    "    \n",
    "\n",
    "    fig3.suptitle(\"Profile of column  \" + col_name, fontsize=25)  #Title for the whole figure\n",
    "    fig_name = 'data_profile_df_HTML/fig_' + col_name\n",
    "    fig3.savefig(fig_name, dpi=50)\n",
    "    plt.close('all')\n",
    "    \n",
    "#     plt.show()\n",
    "\n",
    "df.drop(u'quartiles', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data profile graphs for 'object' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  of  4  completed    UNIQUE_CARRIER\n",
      "2  of  4  completed    TAIL_NUM\n",
      "3  of  4  completed    ORIGIN\n",
      "4  of  4  completed    DEST\n"
     ]
    }
   ],
   "source": [
    "# Get the list of object columns from raw dataframe\n",
    "# get object columns which are not empty\n",
    "obj_cols = [cols for cols in df.columns if is_string_dtype(df[cols]) and len(df[cols].dropna())>0]\n",
    "\n",
    "iter_len = len(obj_cols)\n",
    "\n",
    "\n",
    "# For each object column in the list\n",
    "for x, col_name in enumerate(obj_cols):\n",
    "    print(x+1, \" of \", iter_len, \" completed   \",  col_name)\n",
    "    \n",
    "    # Create a copy of the column values without nulls or NA\n",
    "    no_null_col = df[col_name].dropna()\n",
    "\n",
    "    values_freq_threshold = 25\n",
    "    col_unique_count = df[col_name].nunique()\n",
    "    \n",
    "    # If unique values count is below the threshold value then store the details of unique values\n",
    "    col_unique_vals = df[col_name].value_counts(normalize=True, sort=True)\n",
    "    \n",
    "    # Plot the graphs\n",
    "    fig4 = plt.figure(figsize=(20,7))\n",
    "    fig4.suptitle(\"Profile of column  \" + col_name, fontsize=25)  #Title for the whole figure\n",
    "    plt.subplots_adjust(wspace=0.4, hspace=0.35)\n",
    "\n",
    "    ax1 = fig4.add_subplot(1,1,1)\n",
    "    ax1.set_title(\"Bar chart for top 25 values\", fontsize=20)\n",
    "    plt.setp(ax1.get_xticklabels(), ha=\"right\", rotation=35, fontsize=15)\n",
    "    plt.setp(ax1.get_yticklabels(), ha=\"right\", fontsize=15)\n",
    "    \n",
    "    col_unique_vals.head(values_freq_threshold).sort_values(ascending=False).plot.bar()\n",
    "    for p in ax1.patches:\n",
    "        ax1.annotate(str(round(p.get_height(),2)), (p.get_x() * 1.005, p.get_height() * 1.005), fontsize=15)\n",
    "    \n",
    "    fig4.suptitle(\"Profile of column  \" + col_name, fontsize=25)  #Title for the whole figure\n",
    "    fig_name = 'data_profile_df_HTML/fig_' + col_name\n",
    "    fig4.savefig(fig_name, dpi= 50)\n",
    "\n",
    "    plt.close('all')\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Candidate columns for Category type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysing how many unique values an 'object' column has will be useful to detrmine which columns are good candidates for *Categorical* data type. In combination with the total memory used by 'object' data type and each 'object' data type column, decisions can be made on converting them Category type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a df and a column for % of memory by each object column\n",
    "cardn_df = data_qlt_df[data_qlt_df['col_data_type'] == 'object'][['column_name', 'col_memory', '%_of_dtype_mem', '%_of_total_memory', 'unique_values_count']]\n",
    "\n",
    "cardn_df = cardn_df.sort_values('unique_values_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct the Data Profile Dataframe HTML table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML generated\n"
     ]
    }
   ],
   "source": [
    "cardn_html = '\\\n",
    "<html>\\\n",
    "    <body>\\\n",
    "        <table style=\"table-layout:fixed;\" style=\"text-align:left;\" width=\"1000px\" style=\"border: none;\">'\n",
    "\n",
    "# Main Heading\n",
    "cardn_html = (cardn_html +\\\n",
    "        '<tr >\\\n",
    "        <td colspan=\"6\" style=\"text-align:center\" bgcolor=\"black\">\\\n",
    "        <font size=\"6\" color=\"#fff\">Data Profiling of  ' + F'{raw_data_file}' + '</font></strong></td>'\n",
    "        '</tr>') \n",
    "\n",
    "# Environment Info band\n",
    "cardn_html = (cardn_html +\n",
    "        '<tr >\\\n",
    "        <td colspan=\"6\" style=\"text-align:center\" bgcolor=\"#111E6C\">\\\n",
    "        <font size=\"5\" color=\"#fff\">Execution Environment Information</font></strong></td>'\n",
    "        '</tr>') \n",
    "\n",
    "# Environment details\n",
    "cardn_html = (cardn_html +\n",
    "        '<tr >' \n",
    "        F'<td style=\"text-align:left\" bgcolor=\"#E3F2FD\"><strong>\\\n",
    "        <font color=\"#D37538\">Date Generated : </font><br><font color=\"#000080\">{date_generated.date()}<br></font></strong></td>'\\\n",
    "        F'<td style=\"text-align:left\" bgcolor=\"#E3F2FD\"><strong>\\\n",
    "        <font color=\"#D37538\">Code Version : </font><br><font color=\"#000080\">1.0<br></font></strong></td>'\\\n",
    "        F'<td style=\"text-align:left\" bgcolor=\"#E3F2FD\"><strong>\\\n",
    "        <font color=\"#D37538\">Python Version : </font><br><font color=\"#000080\">{python_version() }</font></strong></td>'\\\n",
    "        F'<td style=\"text-align:left\" bgcolor=\"#E3F2FD\"><strong>\\\n",
    "        <font color=\"#D37538\">Pandas Version : </font><br><font color=\"#000080\">{pd.__version__ }</font></strong></td>'\\\n",
    "        F'<td style=\"text-align:left\" bgcolor=\"#E3F2FD\"><strong>\\\n",
    "        <font color=\"#D37538\">Matplotlib Version : </font><br><font color=\"#000080\"></font></strong></td>'\\\n",
    "        F'<td style=\"text-align:left\" bgcolor=\"#E3F2FD\"><strong>\\\n",
    "        <font color=\"#D37538\">Seaborn Version : </font><br><font color=\"#000080\">Not used</font></strong></td>'\\\n",
    "        '</tr>')\n",
    "\n",
    "# Summary Info band\n",
    "cardn_html = (cardn_html +\n",
    "        '<tr >\\\n",
    "        <td colspan=\"6\" style=\"text-align:center\" bgcolor=\"#111E6C\">\\\n",
    "        <font size=\"5\" color=\"#fff\">Data Profile Summary Information</font></strong></td>'\n",
    "        '</tr>')             \n",
    "\n",
    "# Dataframe shape\n",
    "cardn_html = (cardn_html +\n",
    "        '<tr >' \n",
    "        F'<td colspan=\"3\" style=\"text-align:left\" bgcolor=\"#E3F2FD\"><strong>\\\n",
    "        <font color=\"#D37538\">Dataframe rows : </font><font color=\"#000080\">{df.shape[0] :,}</font></strong></td>'\n",
    "        F'<td colspan=\"3\" style=\"text-align:left\" bgcolor=\"#E3F2FD\"><strong>\\\n",
    "        <font color=\"#D37538\">Dataframe Columns :</font><font color=\"#000080\"> {df.shape[1] :,}</font></strong></td>'\n",
    "        '</tr>')\n",
    "\n",
    "\n",
    "# Disk and Memory size\n",
    "cardn_html = (cardn_html + \\\n",
    "          F'<tr><td colspan=\"6\" style=\"text-align:left\" bgcolor=\"#0F52BA\"><font size=\"3\" color=\"white\"> \\\n",
    "          <strong>File size on disk vs. Dataframe size in memory</strong></font></td></tr>')\n",
    "\n",
    "cardn_html = (cardn_html +\n",
    "        '<tr >\\\n",
    "        <td colspan=\"3\" style=\"text-align:left\" bgcolor=\"#E3F2FD\" >'\\\n",
    "        '<strong>'\\\n",
    "        F'<font color=\"#D37538\">File size in disk : </font><font color=\"#000080\">{file_size :.2f} MB </font><br>'\n",
    "        F'<font color=\"#D37538\">Dataframe size in memory :</font><font color=\"#000080\"> {df_mem :.2f} MB </font><br>'\n",
    "        F'<font color=\"#D37538\">File size increase in memory :</font><font color=\"red\">{sz_increase :.2%} </font></strong><br>'\n",
    "        '</td>')\n",
    "\n",
    "img_src1 = '<img src=\"' + 'fig_df_tot_memory.png' + '\" >'\n",
    "cardn_html = (cardn_html + \n",
    "        '<td colspan=\"3\" style=\"text-align:left\" bgcolor=\"#E3F2FD\">'\\\n",
    "        + F'{img_src1}' + \\\n",
    "        '</td></tr>'   )\n",
    "\n",
    "\n",
    "# Dataframe column types and size in memory\n",
    "cardn_html = (cardn_html + \\\n",
    "          F'<tr><td colspan=\"6\" style=\"text-align:left\" bgcolor=\"#0F52BA\"><font size=\"3\" color=\"white\"> \\\n",
    "          <strong>Dataframe column types and size in memory</strong></font></td></tr>')\n",
    "cardn_html = (cardn_html +\n",
    "        '<tr >' \n",
    "        F'<td colspan=\"3\" style=\"text-align:left\" bgcolor=\"#E3F2FD\"><strong>\\\n",
    "        <font color=\"#000080\">{plt_dtype.reindex().to_html()}</font></strong></td>'\n",
    "        )\n",
    "\n",
    "img_src2 = '<img src=\"' + 'fig_cols_memory.png' + '\" >'\n",
    "cardn_html = (cardn_html + \n",
    "        '<td colspan=\"3\" style=\"text-align:left\" bgcolor=\"#E3F2FD\">'\\\n",
    "        + F'{img_src2}' + \\\n",
    "        '</td></tr>'   )\n",
    "cardn_html = (cardn_html + \n",
    "        '<tr>'\n",
    "        '<td colspan=\"6\" style=\"text-align:left\" bgcolor=\"#E3F2FD\">'\\\n",
    "        + '<font color=\"black\">'  + \"If object (string) data type uses more memory one option is to explore whether there are any possible columns with low cardinality that could be converted to Category data type.\\\n",
    "           If int64 or float64 data type uses high memory then it could be explored whether these can be chnaged to low memory data types of similar data type. e.g float64 to float16\" + \\\n",
    "        '</font></td></tr>'   )\n",
    "\n",
    "# Memory used by 'object' data type\n",
    "cardn_html = (cardn_html + \\\n",
    "          F'<tr><td colspan=\"6\" style=\"text-align:left\" bgcolor=\"#0F52BA\"><font size=\"3\" color=\"white\"> \\\n",
    "          <strong>Memory used by \"object\" data type</strong></font></td></tr>')\n",
    "cardn_html = (cardn_html +\n",
    "        '<tr >' \n",
    "        F'<td colspan=\"6\" style=\"text-align:left\" bgcolor=\"#E3F2FD\"><strong>\\\n",
    "        <font color=\"#000080\">{cardn_df.sort_values(\"unique_values_count\").to_html()}</font></strong></td>'\n",
    "        '</tr>')\n",
    "\n",
    "img_src3 = '<img src=\"' + 'fig_object_cols_memory.png' + '\" >'\n",
    "cardn_html = (cardn_html + \n",
    "        '<tr>'\n",
    "        '<td colspan=\"6\" style=\"text-align:left\" bgcolor=\"#E3F2FD\">'\\\n",
    "        + F'{img_src3}' + \\\n",
    "        '</td></tr>'   )\n",
    "cardn_html = (cardn_html + \n",
    "        '<tr>'\n",
    "        '<td colspan=\"6\" style=\"text-align:left;\" bgcolor=\"#E3F2FD\">'\\\n",
    "        + '<font color=\"black\">' + \"Analysing how many unique values an 'object' column has will be useful to detrmine which columns are good candidates for *Categorical* data type.\\\n",
    "           In combination with the total memory used by 'object' data type and each 'object' data type column, decisions can be made on converting them Category type. <br>\\\n",
    "           Object/string data type columns with low cardinality is suitable for Category type.<br>\\\n",
    "           <strong>The threshold of 'low cardinality' depends on the domain of the data and data usage patterns.</strong>\" + \\\n",
    "        '</font></td></tr>'   )\n",
    "\n",
    "\n",
    "# Memory used by 'non-object' data type\n",
    "cardn_html = (cardn_html + \\\n",
    "          F'<tr><td colspan=\"6\" style=\"text-align:left\" bgcolor=\"#0F52BA\"><font size=\"3\" color=\"white\"> \\\n",
    "          <strong>Memory used by \"Non-Object\" data type</strong></font></td></tr>')\n",
    "# cardn_html = (cardn_html +\n",
    "#         '<tr >' \n",
    "#         F'<td colspan=\"6\" style=\"text-align:left\" bgcolor=\"#E3F2FD\"><strong>\\\n",
    "#         <font color=\"#000080\">{cardn_df.sort_values(\"unique_values_count\").to_html()}</font></strong></td>'\n",
    "#         '</tr>')\n",
    "\n",
    "img_src3 = '<img src=\"' + 'fig_non_object_cols_memory.png' + '\" >'\n",
    "cardn_html = (cardn_html + \n",
    "        '<tr>'\n",
    "        '<td colspan=\"6\" style=\"text-align:left\" bgcolor=\"#E3F2FD\">'\\\n",
    "        + F'{img_src3}' + \\\n",
    "        '</td></tr>'   )\n",
    "cardn_html = (cardn_html + \n",
    "        '<tr>'\n",
    "        '<td colspan=\"6\" style=\"text-align:left;\" bgcolor=\"#E3F2FD\">'\\\n",
    "        + '<font color=\"black\">' + \"By analysing the min and max values of the numeric columns decions can be made to downcast the data type to more memory efficient storage types.\" + \\\n",
    "        '</font></td></tr>'   )\n",
    "\n",
    "\n",
    "# Column details band\n",
    "cardn_html = (cardn_html +\n",
    "        '<tr >\\\n",
    "        <td colspan=\"6\" style=\"text-align:center\" bgcolor=\"#111E6C\">\\\n",
    "        <font size=\"5\" color=\"#fff\">Data Profile Detail Information</font></strong></td>'\n",
    "        '</tr>') \n",
    "\n",
    "# Iterate over the rows of Data Quality Dataframe to print unique values and normalised frequencies\n",
    "for ind, row in data_qlt_df.iterrows():\n",
    "\n",
    "    # Column name\n",
    "    cardn_html = (cardn_html + \\\n",
    "              F'<tr><td colspan=\"6\" style=\"text-align:left\" bgcolor=\"#0F52BA\"><font size=\"3\" color=\"white\"> \\\n",
    "              <strong>{data_qlt_df[\"column_name\"][ind]}</strong></font></td></tr>')\n",
    "    \n",
    "    # Value counts\n",
    "    cardn_html = (cardn_html +\n",
    "            '<tr >' \n",
    "            F'<td style=\"text-align:left;\" bgcolor=\"#73C2FB\"> <strong>\\\n",
    "            <font color=\"#D37538\">Data Type : </font><br><font color=\"#000080\">{data_qlt_df[\"col_data_type\"][ind]}</font></strong></td>'\n",
    "            F'<td style=\"text-align:left\" bgcolor=\"#73C2FB\"><strong>\\\n",
    "            <font color=\"#D37538\">Unique Count :</font><br><font color=\"#000080\"> {data_qlt_df[\"unique_values_count\"][ind] :,}</font></strong></td>'\n",
    "            F'<td style=\"text-align:left\" bgcolor=\"#73C2FB\"><strong>\\\n",
    "            <font color=\"#D37538\">Non-Nulls Count  : </font><br><font color=\"#000080\"> {data_qlt_df[\"non_null_values\"][ind] :,} \\\n",
    "            ({  data_qlt_df[\"%_of_non_nulls\"][ind] :.2f})%</font></strong></td>'\n",
    "            F'<td style=\"text-align:left\" colspan=\"2\" bgcolor=\"#73C2FB\"><strong>\\\n",
    "            <font color=\"#D37538\">Nulls Values Count  : </font><br><font color=\"#000080\"> {data_qlt_df[\"null_values\"][ind] :,} \\\n",
    "            ({  data_qlt_df[\"%_of_nulls\"][ind] :.2f})%</font></strong></td>'\n",
    "           F'<td style=\"text-align:left\" bgcolor=\"#73C2FB\"><strong>\\\n",
    "            <font color=\"#D37538\">Total Count  : </font><br><font color=\"#000080\"> {data_qlt_df[\"count\"][ind] :,}</font></strong></td>'\n",
    "                  '</tr>')\n",
    "\n",
    "    # Memory details\n",
    "    cardn_html = (cardn_html +\n",
    "        '<tr>'\n",
    "        F'<td style=\"text-align:left\" bgcolor=\"#64B5F6\"><strong>\\\n",
    "        <font color=\"#D37538\">Column Memory : </font><br><font color=\"#000080\"> {data_qlt_df[\"col_memory\"][ind] :,.2} MB</font></strong></td>'\n",
    "        F'<td style=\"text-align:left\" colspan=\"2\" bgcolor=\"#64B5F6\"><strong>\\\n",
    "        <font color=\"#D37538\">Column Memory as % of Dtype Memory  : </font><br><font color=\"#000080\"> {data_qlt_df[\"%_of_dtype_mem\"][ind] :.2f}% </td>' \\\n",
    "        F'<td style=\"text-align:left\" colspan=\"3\" bgcolor=\"#64B5F6\"><strong>\\\n",
    "        <font color=\"#D37538\">Column Memory as % of Raw Data DF Memory  : </font><br><font color=\"#000080\"> {data_qlt_df[\"%_of_total_memory\"][ind] :,.2f}%</td>'\n",
    "        '</tr>')\n",
    " \n",
    "    \n",
    "    # Include max. min etc. only for non-object (numeric) columns\n",
    "    if data_qlt_df.iloc[ind]['col_data_type'] != 'object':\n",
    "        # Column statistics\n",
    "        cardn_html = (cardn_html +\n",
    "            '<tr>'\n",
    "            F'<td style=\"text-align:left\" bgcolor=\"#BBDEFB\"><strong>\\\n",
    "            <font color=\"#D37538\">Mean : </font><br><font color=\"#000080\"> {data_qlt_df[\"mean\"][ind] :,.2f}</font></strong></td>'\n",
    "            F'<td style=\"text-align:left\" bgcolor=\"#BBDEFB\"><strong>\\\n",
    "            <font color=\"#D37538\">Min  : </font><br><font color=\"#000080\"> {data_qlt_df[\"min\"][ind]  :,.2f} </font></strong></td>'\n",
    "            F'<td style=\"text-align:left\" bgcolor=\"#BBDEFB\"><strong>\\\n",
    "            <font color=\"#D37538\">25%  : </font><br><font color=\"#000080\"> {data_qlt_df[\"25%\"][ind]  :,.2f} </font></strong></td>'\n",
    "            F'<td style=\"text-align:left\" bgcolor=\"#BBDEFB\"><strong>\\\n",
    "            <font color=\"#D37538\">50%  : </font><br><font color=\"#000080\"> {data_qlt_df[\"50%\"][ind]  :,.2f} </font></strong></td>'\n",
    "            F'<td style=\"text-align:left\" bgcolor=\"#BBDEFB\"><strong>\\\n",
    "            <font color=\"#D37538\">75%  : </font><br><font color=\"#000080\"> {data_qlt_df[\"75%\"][ind]  :,.2f} </font></strong></td>'\n",
    "            F'<td style=\"text-align:left\" bgcolor=\"#BBDEFB\"><strong>\\\n",
    "            <font color=\"#D37538\">Max  : </font><br><font color=\"#000080\"> {data_qlt_df[\"max\"][ind] :,.2f} </font></strong></td>'\n",
    "            '</tr>')\n",
    "    \n",
    "    \n",
    "    img_src = '<img src=\"' + 'fig_' + data_qlt_df['column_name'][ind] + '.png' + '\" width=\"100%\">'\n",
    "    cardn_html = (cardn_html + \n",
    "            '<tr>'\n",
    "            '<td colspan=\"6\" style=\"text-align:left\" bgcolor=\"#E3F2FD\">'\\\n",
    "            + F'{img_src}' + \\\n",
    "            '</td></tr>'   )\n",
    "\n",
    "    \n",
    "    \n",
    "# Footer\n",
    "cardn_html = (cardn_html +\n",
    "        '<tr >\\\n",
    "        <td colspan=\"6\" style=\"text-align:center\" bgcolor=\"black\">\\\n",
    "        <font size=\"2\" color=\"#fff\">Author: Anandakumar Varatharajah  (www.AnalyticsInsights.ninja)</font></strong></td>'\n",
    "        '</tr>') \n",
    "\n",
    "# HTML table close\n",
    "cardn_html = (cardn_html + '</table></body>')\n",
    "\n",
    "\n",
    "# Write the HTML file to disk\n",
    "with open(\"data_profile_df_HTML/data_profile_dataframe.html\",\"w+\") as f:\n",
    "    f.write(cardn_html)\n",
    "\n",
    "# Display the Data Profile Dataframe HTML table\n",
    "# HTML(cardn_html)\n",
    "print(\"HTML generated\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
